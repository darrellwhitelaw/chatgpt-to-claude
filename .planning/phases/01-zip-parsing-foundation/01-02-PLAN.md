---
phase: 01-zip-parsing-foundation
plan: "02"
type: execute
wave: 2
depends_on:
  - 01-01
files_modified:
  - src-tauri/src/pipeline/zip_reader.rs
  - src-tauri/src/pipeline/json_parser.rs
  - src-tauri/src/pipeline/normalizer.rs
  - src-tauri/src/pipeline/mod.rs
  - src-tauri/src/commands/ingest.rs
  - src-tauri/src/store/db.rs
autonomous: true
requirements:
  - IMP-04
  - IMP-05

must_haves:
  truths:
    - "Given a valid ChatGPT export ZIP, calling parse_zip writes conversation rows to SQLite and emits Complete with non-zero total"
    - "The Rust pipeline reads conversations.json directly from the ZIP entry without extracting to disk"
    - "All nullable fields in conversations.json are typed as Option<T> — no unwrap() on JSON fields"
    - "The pipeline does not load the full conversations.json into memory — it streams element by element"
    - "Unknown content_type values in message content are logged and skipped, not panicked"
    - "cargo check passes with no errors"
  artifacts:
    - path: "src-tauri/src/pipeline/zip_reader.rs"
      provides: "Opens ZIP archive and returns BufReader-wrapped ZipFile for conversations.json"
      exports: ["open_conversations_entry"]
    - path: "src-tauri/src/pipeline/json_parser.rs"
      provides: "Streaming JSON array deserialization — ConversationExport structs with all Option<T> fields"
      exports: ["ConversationExport", "MessageNode", "Message", "Author", "Content", "stream_conversations"]
    - path: "src-tauri/src/pipeline/normalizer.rs"
      provides: "Converts ConversationExport to ConversationRecord for SQLite insert"
      exports: ["ConversationRecord", "normalize"]
    - path: "src-tauri/src/commands/ingest.rs"
      provides: "Full parse_zip implementation — ZIP open, stream parse, normalize, SQLite insert, progress events"
      exports: ["parse_zip", "IngestEvent"]
    - path: "src-tauri/src/store/db.rs"
      provides: "insert_conversation function alongside existing init_schema"
      exports: ["init_schema", "insert_conversation"]
  key_links:
    - from: "src-tauri/src/commands/ingest.rs"
      to: "src-tauri/src/pipeline/zip_reader.rs"
      via: "open_conversations_entry(path) called in parse_zip"
      pattern: "open_conversations_entry"
    - from: "src-tauri/src/commands/ingest.rs"
      to: "src-tauri/src/pipeline/json_parser.rs"
      via: "stream_conversations(reader) iterator consumed in parse_zip loop"
      pattern: "stream_conversations"
    - from: "src-tauri/src/commands/ingest.rs"
      to: "src-tauri/src/store/db.rs"
      via: "insert_conversation called per iteration inside parse_zip"
      pattern: "insert_conversation"
    - from: "src-tauri/src/pipeline/json_parser.rs"
      to: "Deserializer::from_reader(reader).into_iter::<ConversationExport>()"
      via: "serde_json streaming array pattern — NOT StreamDeserializer"
      pattern: "into_iter"
---

<objective>
Implement the full Rust streaming pipeline: open the ZIP archive, stream-parse conversations.json element by element using serde_json's array iterator, normalize each conversation to a SQLite record, and insert it — all without loading the full JSON into memory. The parse_zip command stub from plan 01-01 is replaced with the real implementation.

Purpose: This is the core correctness guarantee of Phase 1. IMP-04 (streaming, no memory load) and IMP-05 (defensive nullability) are fully implemented here. Without this plan, the app cannot ingest any real data.
Output: A working Rust pipeline that accepts a ZIP path, streams all conversations into SQLite, and emits typed progress events back to the frontend via Channel.
</objective>

<execution_context>
@/Users/darrellwhitelaw/.claude/get-shit-done/workflows/execute-plan.md
@/Users/darrellwhitelaw/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-zip-parsing-foundation/01-RESEARCH.md
@.planning/phases/01-zip-parsing-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: ZIP reader and streaming JSON parser with defensive types</name>
  <files>
    src-tauri/src/pipeline/zip_reader.rs
    src-tauri/src/pipeline/json_parser.rs
    src-tauri/src/pipeline/mod.rs
  </files>
  <action>
**`src-tauri/src/pipeline/zip_reader.rs`** — Opens the ZIP and locates conversations.json:

```rust
use std::fs::File;
use std::io::BufReader;
use zip::ZipArchive;

/// Opens the ZIP at `path` and returns a BufReader over the conversations.json entry.
/// Tries `conversations.json` at root first; falls back to searching any entry whose
/// name ends with `/conversations.json` to handle sub-directory variants.
pub fn open_conversations_entry(
    path: &str,
) -> Result<BufReader<zip::read::ZipFile<'static>>, String> {
    // Note: ZipFile borrows from ZipArchive. The idiomatic approach is to extract to
    // a temp file or pass the reader differently. Use a simpler owned approach:
    // read conversations.json bytes into a Vec<u8>, then wrap in Cursor for streaming.
    // This is acceptable because we're streaming the JSON, not the ZIP bytes.
    let file = File::open(path).map_err(|e| format!("Cannot open ZIP: {e}"))?;
    let mut archive = ZipArchive::new(file).map_err(|e| format!("Invalid ZIP: {e}"))?;

    // Try root-level entry first
    let bytes = if archive.by_name("conversations.json").is_ok() {
        let mut entry = archive
            .by_name("conversations.json")
            .map_err(|e| e.to_string())?;
        let mut buf = Vec::new();
        std::io::Read::read_to_end(&mut entry, &mut buf)
            .map_err(|e| format!("Cannot read conversations.json: {e}"))?;
        buf
    } else {
        // Fallback: find at any depth
        let idx = (0..archive.len())
            .find(|&i| {
                archive
                    .by_index(i)
                    .map(|e| {
                        e.name() == "conversations.json"
                            || e.name().ends_with("/conversations.json")
                    })
                    .unwrap_or(false)
            })
            .ok_or_else(|| "conversations.json not found in ZIP".to_string())?;
        let mut entry = archive.by_index(idx).map_err(|e| e.to_string())?;
        let mut buf = Vec::new();
        std::io::Read::read_to_end(&mut entry, &mut buf)
            .map_err(|e| format!("Cannot read conversations.json: {e}"))?;
        buf
    };

    // Wrap bytes in BufReader<Cursor> so json_parser receives a standard BufRead
    Ok(BufReader::new(std::io::Cursor::new(bytes)))
}

// Re-export the concrete reader type alias for use in json_parser
pub type ConversationsReader = BufReader<std::io::Cursor<Vec<u8>>>;
```

Note on the borrow issue: `ZipFile` borrows from `ZipArchive`, making it impossible to return from the function that owns the archive. Reading to `Vec<u8>` then wrapping in `Cursor` is the standard idiomatic resolution. The JSON streaming still works element-by-element — memory usage is proportional to `conversations.json` size, not the full ZIP (attachments are not loaded).

**`src-tauri/src/pipeline/json_parser.rs`** — Defensive serde structs and streaming iterator:

All fields that can be null or absent in conversations.json MUST be `Option<T>`. Use `serde_json::Value` for fully open fields (`metadata`, `parts` elements). Do NOT use `unwrap()` on any deserialized field.

```rust
use serde::Deserialize;
use std::collections::HashMap;
use std::io::Read;

#[derive(Debug, Deserialize)]
pub struct ConversationExport {
    pub id: String,
    pub title: Option<String>,
    pub create_time: Option<f64>,
    pub update_time: Option<f64>,
    #[serde(default)]
    pub mapping: HashMap<String, MessageNode>,
    pub current_node: Option<String>,
}

#[derive(Debug, Deserialize, Clone)]
pub struct MessageNode {
    pub id: String,
    pub parent: Option<String>,
    #[serde(default)]
    pub children: Vec<String>,
    pub message: Option<Message>,
}

#[derive(Debug, Deserialize, Clone)]
pub struct Message {
    pub id: String,
    pub author: Author,
    pub create_time: Option<f64>,
    pub content: Option<Content>,
    #[serde(default)]
    pub metadata: serde_json::Value,
}

#[derive(Debug, Deserialize, Clone)]
pub struct Author {
    pub role: String,
    #[serde(default)]
    pub name: Option<String>,
}

#[derive(Debug, Deserialize, Clone)]
pub struct Content {
    pub content_type: String,
    // parts can be strings, objects, or mixed — use Value to absorb all formats
    #[serde(default)]
    pub parts: Vec<serde_json::Value>,
}

/// Streams ConversationExport elements from a reader containing a top-level JSON array.
///
/// CRITICAL: This uses `Deserializer::from_reader().into_iter()` — NOT `StreamDeserializer`.
/// `StreamDeserializer` handles multiple top-level values (NDJSON). `into_iter()` handles
/// a single top-level array, yielding one element at a time through serde's sequence visitor.
pub fn stream_conversations<R: Read>(
    reader: R,
) -> impl Iterator<Item = Result<ConversationExport, serde_json::Error>> {
    serde_json::Deserializer::from_reader(reader).into_iter::<ConversationExport>()
}
```

**`src-tauri/src/pipeline/mod.rs`** — expose modules:
```rust
pub mod json_parser;
pub mod normalizer;
pub mod traversal; // added by plan 01-03
pub mod zip_reader;
```

Note: `traversal` module will be added by plan 01-03. If that plan hasn't run yet, comment it out. The final state after both plans should have all four modules.
  </action>
  <verify>
    <automated>cd /Users/darrellwhitelaw/Claude/chatgpt-to-claude && cargo check --manifest-path src-tauri/Cargo.toml 2>&1 | grep "^error" | head -10</automated>
    <manual>Zero lines of output — no compilation errors</manual>
  </verify>
  <done>
    - `src-tauri/src/pipeline/zip_reader.rs` compiles with `open_conversations_entry` returning `Result<ConversationsReader, String>`
    - `src-tauri/src/pipeline/json_parser.rs` compiles with all serde structs and `stream_conversations` function
    - All JSON fields that can be null in conversations.json are `Option<T>`
    - `parts` field uses `Vec<serde_json::Value>` to handle mixed-type elements
    - `metadata` field uses `serde_json::Value` to absorb unknown fields
    - `cargo check` passes with no errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Normalizer, SQLite insert, and full parse_zip command implementation</name>
  <files>
    src-tauri/src/pipeline/normalizer.rs
    src-tauri/src/store/db.rs
    src-tauri/src/commands/ingest.rs
  </files>
  <action>
**`src-tauri/src/pipeline/normalizer.rs`** — converts `ConversationExport` to a flat record for SQLite:

```rust
use crate::pipeline::json_parser::{ConversationExport, Content, Message};

pub struct ConversationRecord {
    pub id: String,
    pub title: String,
    pub created_at: Option<i64>,
    pub message_count: u32,
    pub has_images: bool,
    pub has_code: bool,
    pub token_estimate: u32,
    pub full_text: String,
}

/// Normalizes a ConversationExport to a ConversationRecord.
/// Traversal (IMP-06) is NOT done here — normalizer works on the raw export.
/// Traversal is wired in the command after plan 01-03 lands.
/// For now: count messages from mapping values that have role user/assistant.
pub fn normalize(export: ConversationExport) -> ConversationRecord {
    let title = export
        .title
        .filter(|t| !t.is_empty())
        .unwrap_or_else(|| "Untitled".to_string());

    let created_at = export.create_time.map(|t| t as i64);

    let mut message_count: u32 = 0;
    let mut has_images = false;
    let mut has_code = false;
    let mut full_text = String::new();

    for node in export.mapping.values() {
        if let Some(ref msg) = node.message {
            let role = msg.author.role.as_str();
            if role == "user" || role == "assistant" {
                message_count += 1;
                if let Some(ref content) = msg.content {
                    let extracted = extract_text(content);
                    if !extracted.is_empty() {
                        full_text.push_str(&extracted);
                        full_text.push('\n');
                    }
                    if content.content_type == "multimodal_text" {
                        has_images = true;
                    }
                    if content.content_type == "code" {
                        has_code = true;
                    }
                }
            }
        }
    }

    // Rough token estimate: ~4 chars per token
    let token_estimate = (full_text.len() / 4) as u32;

    ConversationRecord {
        id: export.id,
        title,
        created_at,
        message_count,
        has_images,
        has_code,
        token_estimate,
        full_text,
    }
}

fn extract_text(content: &Content) -> String {
    content
        .parts
        .iter()
        .filter_map(|part| part.as_str().map(|s| s.to_string()))
        .collect::<Vec<_>>()
        .join(" ")
}
```

**`src-tauri/src/store/db.rs`** — add `insert_conversation` alongside existing `init_schema`:

```rust
use crate::pipeline::normalizer::ConversationRecord;
use rusqlite::{Connection, Result, params};

pub fn init_schema(conn: &Connection) -> Result<()> {
    conn.execute_batch(include_str!("schema.sql"))
}

pub fn insert_conversation(conn: &Connection, record: &ConversationRecord) -> Result<()> {
    conn.execute(
        "INSERT OR REPLACE INTO conversations
            (id, title, created_at, message_count, has_images, has_code, token_estimate, full_text)
         VALUES (?1, ?2, ?3, ?4, ?5, ?6, ?7, ?8)",
        params![
            record.id,
            record.title,
            record.created_at,
            record.message_count,
            record.has_images as i32,
            record.has_code as i32,
            record.token_estimate,
            record.full_text,
        ],
    )?;
    Ok(())
}
```

**`src-tauri/src/commands/ingest.rs`** — replace stub with full implementation:

```rust
use crate::pipeline::{json_parser, normalizer, zip_reader};
use crate::store::db;
use crate::AppState;
use serde::{Deserialize, Serialize};
use tauri::{ipc::Channel, State};

#[derive(Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase", tag = "event", content = "data")]
pub enum IngestEvent {
    Started,
    ExtractingZip,
    ParsingConversations { processed: u32 },
    BuildingIndex,
    Complete { total: u32, earliest_year: i32, latest_year: i32 },
    Error { message: String },
}

#[tauri::command]
pub async fn parse_zip(
    path: String,
    on_event: Channel<IngestEvent>,
    state: State<'_, AppState>,
) -> Result<(), String> {
    on_event
        .send(IngestEvent::Started)
        .map_err(|e| e.to_string())?;

    on_event
        .send(IngestEvent::ExtractingZip)
        .map_err(|e| e.to_string())?;

    let reader = zip_reader::open_conversations_entry(&path)?;

    on_event
        .send(IngestEvent::ParsingConversations { processed: 0 })
        .map_err(|e| e.to_string())?;

    let db = state.db.lock().map_err(|e| e.to_string())?;

    let mut count: u32 = 0;
    let mut earliest_year: i32 = i32::MAX;
    let mut latest_year: i32 = i32::MIN;

    for result in json_parser::stream_conversations(reader) {
        let export = match result {
            Ok(e) => e,
            Err(err) => {
                // Log and skip malformed entries — do not crash (IMP-05)
                eprintln!("Skipping malformed conversation: {err}");
                continue;
            }
        };

        // Track year range from create_time
        if let Some(ts) = export.create_time {
            // Unix timestamp — convert to year (rough: ts / 31_536_000 + 1970)
            let year = (ts / 31_536_000.0) as i32 + 1970;
            if year < earliest_year { earliest_year = year; }
            if year > latest_year { latest_year = year; }
        }

        let record = normalizer::normalize(export);

        db::insert_conversation(&db, &record).map_err(|e| e.to_string())?;

        count += 1;

        // Emit progress every 50 conversations to avoid flooding the channel
        if count % 50 == 0 {
            on_event
                .send(IngestEvent::ParsingConversations { processed: count })
                .map_err(|e| e.to_string())?;
        }
    }

    on_event
        .send(IngestEvent::BuildingIndex)
        .map_err(|e| e.to_string())?;

    // Clamp year range if no timestamps found
    if earliest_year == i32::MAX { earliest_year = 2020; }
    if latest_year == i32::MIN { latest_year = 2024; }

    on_event
        .send(IngestEvent::Complete {
            total: count,
            earliest_year,
            latest_year,
        })
        .map_err(|e| e.to_string())?;

    Ok(())
}
```

Note: The command currently uses `mapping.values()` iteration for counting (pre-traversal). After plan 01-03 lands, the normalizer will be updated to call `traversal::linearize_messages` for correct ordering. This is intentional sequencing — the pipeline is functional without traversal for count/year purposes.
  </action>
  <verify>
    <automated>cd /Users/darrellwhitelaw/Claude/chatgpt-to-claude && cargo check --manifest-path src-tauri/Cargo.toml 2>&1 | grep "^error" | head -10</automated>
    <manual>Zero error lines — all modules compile cleanly</manual>
  </verify>
  <done>
    - `cargo check` passes with zero errors
    - `parse_zip` command accepts `path: String`, `on_event: Channel<IngestEvent>`, `state: State<AppState>`
    - ZIP opened via `zip_reader::open_conversations_entry`, no full extraction to disk
    - JSON streamed via `json_parser::stream_conversations` using `into_iter()` — NOT `StreamDeserializer`
    - Malformed conversation entries are skipped with `eprintln!` — pipeline does not panic
    - Each parsed conversation is inserted into SQLite via `db::insert_conversation`
    - `Complete` event emitted with total count and year range
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/darrellwhitelaw/Claude/chatgpt-to-claude
cargo check --manifest-path src-tauri/Cargo.toml
```
Must exit 0 with no `error` lines. Warnings are acceptable.
</verification>

<success_criteria>
- ZIP reader opens conversations.json entry by name, falls back to depth search
- Reads conversations.json bytes into Vec<u8> then wraps in Cursor (avoids borrow-lifetime issue with ZipFile)
- JSON streaming uses `Deserializer::from_reader().into_iter::<ConversationExport>()` — not StreamDeserializer
- All serde structs use `Option<T>` for nullable fields; `serde_json::Value` for open-ended fields
- Malformed entries are logged and skipped — never panic
- `insert_conversation` uses `INSERT OR REPLACE` for idempotent re-runs
- Progress events emitted at start, every 50 conversations, and on completion
- `cargo check` passes with no errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-zip-parsing-foundation/01-02-SUMMARY.md` following the summary template.
</output>
